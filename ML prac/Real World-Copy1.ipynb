{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "27131420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp\n",
    "import seaborn as sns\n",
    "#from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4f055610",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('train_pp.csv')\n",
    "X=data.drop('Survived',axis=1).to_numpy()\n",
    "y=data['Survived'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "21bd6fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801, 7)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e4138637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPNN:\n",
    "    def __init__(self,X,y,lr):\n",
    "        self.inp=X\n",
    "        self.wts=[np.random.rand(self.inp.shape[1],128),\n",
    "                  np.random.rand(128,64),\n",
    "                  np.random.rand(64,32),\n",
    "                  np.random.rand(32,16),\n",
    "                  np.random.rand(16,1)]\n",
    "        self.base=[np.zeros((1,128)),\n",
    "                   np.zeros((1,64)),\n",
    "                   np.zeros((1,32)),\n",
    "                   np.zeros((1,16)),\n",
    "                   np.zeros((1,1))]\n",
    "        self.gamma=[np.ones((1,128)),\n",
    "                   np.ones((1,64)),\n",
    "                   np.ones((1,32)),\n",
    "                   np.ones((1,16))]\n",
    "        self.beta=[np.zeros((1,128)),\n",
    "                   np.zeros((1,64)),\n",
    "                   np.zeros((1,32)),\n",
    "                   np.zeros((1,16))]\n",
    "        \n",
    "        self.lr=lr\n",
    "        self.target=y\n",
    "        self.layers=len(self.wts)-1\n",
    "        self.total_loss=list()\n",
    "\n",
    "    def batchnorm_forward(self,x, gamma, beta, eps=1e-5):\n",
    "        N, D = x.shape\n",
    "        sample_mean = np.mean(x,axis=0)\n",
    "        sample_var = np.var(x,axis=0)\n",
    "        std = np.sqrt(sample_var + eps)\n",
    "        x_centered = x - sample_mean\n",
    "        x_norm = x_centered / std\n",
    "        out = (gamma * x_norm) + beta\n",
    "        cache = (x_norm, x_centered, std, gamma)\n",
    "        return out, cache\n",
    "    \n",
    "    #FeedForward\n",
    "    def FF(self,xin,a,z,cache):\n",
    "        for i in range(self.layers+1):\n",
    "            z[i]=np.matmul(xin,self.wts[i])+self.base[i]\n",
    "            \n",
    "            '''if i reaches the output layer, then sigmoid function will get applied instead  of relu'''\n",
    "            if i==self.layers:\n",
    "                a[i]=self.sigmoid(z[i])\n",
    "            else:\n",
    "                '''batch normalizing z here and storing cache value for a specific batch for all hidden layers,\n",
    "                gamma and beta will get updated based on this cache'''\n",
    "                z[i],cache[i]=self.batchnorm_forward(z[i],self.gamma[i],self.beta[i])\n",
    "                a[i]=self.relu(z[i])\n",
    "                \n",
    "            '''setting up input for the next layer'''\n",
    "            xin=a[i]\n",
    "        return a,z,cache\n",
    "    \n",
    "    def loss(self,y_net,y_cap,batch):\n",
    "        \n",
    "        '''y_net is predicted y, since loss function is not valid for y_net==1 or 0\n",
    "        therefore clipping it with eps\n",
    "        This function return error with shape(1,batch)'''\n",
    "        eps=1e-15\n",
    "        y_net=np.maximum(eps,np.minimum(1-eps,y_net))\n",
    "        e=-((y_cap)*np.log(y_net)+(1-y_cap)*np.log(1-y_net))/batch\n",
    "        return e\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return self.total_loss\n",
    "        \n",
    "    def BP(self,e,delta,z):\n",
    "        \n",
    "        '''updating delta'''\n",
    "        for i in range(self.layers-1,-1,-1):\n",
    "            delta[i]=self.df_relu(z[i])*np.matmul(e,self.wts[i+1].T)            \n",
    "            e=delta[i]\n",
    "        return delta\n",
    "    \n",
    "    def batchnorm_backward(self,dout, cache):\n",
    "        N = dout.shape[0]\n",
    "        x_norm, x_centered, std, gamma = cache\n",
    "        dgamma = np.sum((dout * x_norm),axis=0,keepdims=True)\n",
    "        dbeta = np.sum(dout,axis=0,keepdims=True)\n",
    "        dx_norm = dout * gamma\n",
    "        dx_centered = dx_norm / std\n",
    "        dmean = -(dx_centered.sum(axis=0) + 2/N * x_centered.sum(axis=0))\n",
    "        dstd = (dx_norm * x_centered * -std**(-2)).sum(axis=0)\n",
    "        dvar = dstd / 2 / std\n",
    "        dx = dx_centered + (dmean + dvar * 2 * x_centered) / N\n",
    "        return dx, dgamma, dbeta\n",
    "                    \n",
    "    def update(self,e,x_cap,a,delta,batch,cache):\n",
    "        for i in range(self.layers,0,-1):\n",
    "            \n",
    "            '''updating weigths and bias here\n",
    "            both gradients are average, taken for the batch'''\n",
    "            self.wts[i]-=self.lr*np.matmul(a[i-1].T,e)\n",
    "            self.base[i]-=self.lr*np.sum(e,axis=0,keepdims=True)\n",
    "            e=delta[i-1]\n",
    "            \n",
    "            '''updating gamma and beta here by doing backpropagation for batch normalization'''\n",
    "            dx,dgamma,dbeta=self.batchnorm_backward(e,cache[i-1])\n",
    "            self.gamma[i-1]-=self.lr*dgamma\n",
    "            self.beta[i-1]-=self.lr*dbeta\n",
    "            \n",
    "        '''updating weights and bias for the 1st hidden layer'''\n",
    "        self.wts[0]-=self.lr*np.matmul(x_cap.T,e)\n",
    "        self.base[0]-=self.lr*np.sum(e,axis=0,keepdims=True)\n",
    "        \n",
    "    def relu(self,x):\n",
    "        x[x<0]=0\n",
    "        return x\n",
    "        \n",
    "    def df_relu(self,x):\n",
    "        x[x>=0]=1\n",
    "        x[x<0]=0\n",
    "        return x        \n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "       \n",
    "    \n",
    "    def df_sigmoid(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))    \n",
    "    \n",
    "    def train(self, epoch=20, batch=32):\n",
    "        for i in range(epoch):\n",
    "            print('------------------------------------')\n",
    "            print(f'epoch: {i+1}/{epoch}')\n",
    "            \n",
    "            '''a,z,cache and delta will get initialized everytime for a new epoch'''\n",
    "            loss_per_epo=list()\n",
    "            a=[0]*(self.layers+1)\n",
    "            z=[0]*(self.layers+1)\n",
    "            delta=[0]*(self.layers)\n",
    "            cache=[0]*(self.layers)\n",
    "            \n",
    "            '''providing input for 1 batch in 1 loop iteration'''\n",
    "            for j in range(0,self.inp.shape[0],batch):\n",
    "                x_cap=self.inp[j:j+batch]\n",
    "                y_cap=self.target[j:j+batch].reshape(len(self.target[j:j+batch]),1)\n",
    "                a,z,cache=self.FF(x_cap,a,z,cache)\n",
    "                error=self.loss(a[-1],y_cap,batch)\n",
    "                loss_per_epo.append(sum(error))\n",
    "                delta=self.BP(error,delta,z)\n",
    "                self.update(error,x_cap,a,delta,batch,cache)\n",
    "#                 print(f'error:{np.concatenate((a[-1],y_cap,x_cap),axis=1),error}')\n",
    "\n",
    "            '''calculating loss over an epoch in loss_avg to plot it'''\n",
    "            loss_avg=sum(loss_per_epo)/(self.inp.shape[0]//batch)\n",
    "            self.total_loss.append(loss_avg)\n",
    "            print(f'loss: {loss_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "af9271e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogation\n",
    "cyc=100\n",
    "BP_nn=BPNN(X_train,y_train,lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b14a9417-3c89-49cf-95f9-064aa7ae94b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2,-1000]])\n",
    "BP_nn.relu(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dfa3248d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "epoch: 1/100\n",
      "loss: [1.46131726]\n",
      "------------------------------------\n",
      "epoch: 2/100\n",
      "loss: [0.92192737]\n",
      "------------------------------------\n",
      "epoch: 3/100\n",
      "loss: [0.69306708]\n",
      "------------------------------------\n",
      "epoch: 4/100\n",
      "loss: [0.69391348]\n",
      "------------------------------------\n",
      "epoch: 5/100\n",
      "loss: [0.69389303]\n",
      "------------------------------------\n",
      "epoch: 6/100\n",
      "loss: [0.69387259]\n",
      "------------------------------------\n",
      "epoch: 7/100\n",
      "loss: [0.69385215]\n",
      "------------------------------------\n",
      "epoch: 8/100\n",
      "loss: [0.69383173]\n",
      "------------------------------------\n",
      "epoch: 9/100\n",
      "loss: [0.69381131]\n",
      "------------------------------------\n",
      "epoch: 10/100\n",
      "loss: [0.6937909]\n",
      "------------------------------------\n",
      "epoch: 11/100\n",
      "loss: [0.69377049]\n",
      "------------------------------------\n",
      "epoch: 12/100\n",
      "loss: [0.6937501]\n",
      "------------------------------------\n",
      "epoch: 13/100\n",
      "loss: [0.69372971]\n",
      "------------------------------------\n",
      "epoch: 14/100\n",
      "loss: [0.69370934]\n",
      "------------------------------------\n",
      "epoch: 15/100\n",
      "loss: [0.69368897]\n",
      "------------------------------------\n",
      "epoch: 16/100\n",
      "loss: [0.6936686]\n",
      "------------------------------------\n",
      "epoch: 17/100\n",
      "loss: [0.69364825]\n",
      "------------------------------------\n",
      "epoch: 18/100\n",
      "loss: [0.69362791]\n",
      "------------------------------------\n",
      "epoch: 19/100\n",
      "loss: [0.69360757]\n",
      "------------------------------------\n",
      "epoch: 20/100\n",
      "loss: [0.69358724]\n",
      "------------------------------------\n",
      "epoch: 21/100\n",
      "loss: [0.69356692]\n",
      "------------------------------------\n",
      "epoch: 22/100\n",
      "loss: [0.69354661]\n",
      "------------------------------------\n",
      "epoch: 23/100\n",
      "loss: [0.6935263]\n",
      "------------------------------------\n",
      "epoch: 24/100\n",
      "loss: [0.693506]\n",
      "------------------------------------\n",
      "epoch: 25/100\n",
      "loss: [0.69348572]\n",
      "------------------------------------\n",
      "epoch: 26/100\n",
      "loss: [0.69346543]\n",
      "------------------------------------\n",
      "epoch: 27/100\n",
      "loss: [0.69344516]\n",
      "------------------------------------\n",
      "epoch: 28/100\n",
      "loss: [0.6934249]\n",
      "------------------------------------\n",
      "epoch: 29/100\n",
      "loss: [0.69340464]\n",
      "------------------------------------\n",
      "epoch: 30/100\n",
      "loss: [0.69338439]\n",
      "------------------------------------\n",
      "epoch: 31/100\n",
      "loss: [0.69336415]\n",
      "------------------------------------\n",
      "epoch: 32/100\n",
      "loss: [0.69334392]\n",
      "------------------------------------\n",
      "epoch: 33/100\n",
      "loss: [0.6933237]\n",
      "------------------------------------\n",
      "epoch: 34/100\n",
      "loss: [0.69330348]\n",
      "------------------------------------\n",
      "epoch: 35/100\n",
      "loss: [0.69328328]\n",
      "------------------------------------\n",
      "epoch: 36/100\n",
      "loss: [0.69326308]\n",
      "------------------------------------\n",
      "epoch: 37/100\n",
      "loss: [0.69324289]\n",
      "------------------------------------\n",
      "epoch: 38/100\n",
      "loss: [0.6932227]\n",
      "------------------------------------\n",
      "epoch: 39/100\n",
      "loss: [0.69320253]\n",
      "------------------------------------\n",
      "epoch: 40/100\n",
      "loss: [0.69318236]\n",
      "------------------------------------\n",
      "epoch: 41/100\n",
      "loss: [0.6931622]\n",
      "------------------------------------\n",
      "epoch: 42/100\n",
      "loss: [0.69314205]\n",
      "------------------------------------\n",
      "epoch: 43/100\n",
      "loss: [0.69312191]\n",
      "------------------------------------\n",
      "epoch: 44/100\n",
      "loss: [0.69310177]\n",
      "------------------------------------\n",
      "epoch: 45/100\n",
      "loss: [0.69308165]\n",
      "------------------------------------\n",
      "epoch: 46/100\n",
      "loss: [0.69306153]\n",
      "------------------------------------\n",
      "epoch: 47/100\n",
      "loss: [0.69304142]\n",
      "------------------------------------\n",
      "epoch: 48/100\n",
      "loss: [0.69302132]\n",
      "------------------------------------\n",
      "epoch: 49/100\n",
      "loss: [0.69300122]\n",
      "------------------------------------\n",
      "epoch: 50/100\n",
      "loss: [0.69298114]\n",
      "------------------------------------\n",
      "epoch: 51/100\n",
      "loss: [0.69296106]\n",
      "------------------------------------\n",
      "epoch: 52/100\n",
      "loss: [0.69294099]\n",
      "------------------------------------\n",
      "epoch: 53/100\n",
      "loss: [0.69292093]\n",
      "------------------------------------\n",
      "epoch: 54/100\n",
      "loss: [0.69290088]\n",
      "------------------------------------\n",
      "epoch: 55/100\n",
      "loss: [0.69288083]\n",
      "------------------------------------\n",
      "epoch: 56/100\n",
      "loss: [0.69286079]\n",
      "------------------------------------\n",
      "epoch: 57/100\n",
      "loss: [0.69284076]\n",
      "------------------------------------\n",
      "epoch: 58/100\n",
      "loss: [0.69282074]\n",
      "------------------------------------\n",
      "epoch: 59/100\n",
      "loss: [0.69280073]\n",
      "------------------------------------\n",
      "epoch: 60/100\n",
      "loss: [0.69278072]\n",
      "------------------------------------\n",
      "epoch: 61/100\n",
      "loss: [0.69276073]\n",
      "------------------------------------\n",
      "epoch: 62/100\n",
      "loss: [0.69274074]\n",
      "------------------------------------\n",
      "epoch: 63/100\n",
      "loss: [0.69272076]\n",
      "------------------------------------\n",
      "epoch: 64/100\n",
      "loss: [0.69270078]\n",
      "------------------------------------\n",
      "epoch: 65/100\n",
      "loss: [0.69268082]\n",
      "------------------------------------\n",
      "epoch: 66/100\n",
      "loss: [0.69266086]\n",
      "------------------------------------\n",
      "epoch: 67/100\n",
      "loss: [0.69264092]\n",
      "------------------------------------\n",
      "epoch: 68/100\n",
      "loss: [0.69174702]\n",
      "------------------------------------\n",
      "epoch: 69/100\n",
      "loss: [0.68910486]\n",
      "------------------------------------\n",
      "epoch: 70/100\n",
      "loss: [0.94906096]\n",
      "------------------------------------\n",
      "epoch: 71/100\n",
      "loss: [1.45397696]\n",
      "------------------------------------\n",
      "epoch: 72/100\n",
      "loss: [2.1551789]\n",
      "------------------------------------\n",
      "epoch: 73/100\n",
      "loss: [2.30074696]\n",
      "------------------------------------\n",
      "epoch: 74/100\n",
      "loss: [2.15412839]\n",
      "------------------------------------\n",
      "epoch: 75/100\n",
      "loss: [2.26683448]\n",
      "------------------------------------\n",
      "epoch: 76/100\n",
      "loss: [2.09089871]\n",
      "------------------------------------\n",
      "epoch: 77/100\n",
      "loss: [2.08213645]\n",
      "------------------------------------\n",
      "epoch: 78/100\n",
      "loss: [1.9606714]\n",
      "------------------------------------\n",
      "epoch: 79/100\n",
      "loss: [1.79615409]\n",
      "------------------------------------\n",
      "epoch: 80/100\n",
      "loss: [1.74248273]\n",
      "------------------------------------\n",
      "epoch: 81/100\n",
      "loss: [1.558327]\n",
      "------------------------------------\n",
      "epoch: 82/100\n",
      "loss: [1.67026855]\n",
      "------------------------------------\n",
      "epoch: 83/100\n",
      "loss: [1.41432743]\n",
      "------------------------------------\n",
      "epoch: 84/100\n",
      "loss: [1.49708574]\n",
      "------------------------------------\n",
      "epoch: 85/100\n",
      "loss: [1.50898015]\n",
      "------------------------------------\n",
      "epoch: 86/100\n",
      "loss: [1.4576968]\n",
      "------------------------------------\n",
      "epoch: 87/100\n",
      "loss: [1.48566636]\n",
      "------------------------------------\n",
      "epoch: 88/100\n",
      "loss: [1.39054532]\n",
      "------------------------------------\n",
      "epoch: 89/100\n",
      "loss: [1.43968918]\n",
      "------------------------------------\n",
      "epoch: 90/100\n",
      "loss: [1.4561476]\n",
      "------------------------------------\n",
      "epoch: 91/100\n",
      "loss: [1.41377332]\n",
      "------------------------------------\n",
      "epoch: 92/100\n",
      "loss: [1.43863256]\n",
      "------------------------------------\n",
      "epoch: 93/100\n",
      "loss: [1.42937064]\n",
      "------------------------------------\n",
      "epoch: 94/100\n",
      "loss: [1.41217471]\n",
      "------------------------------------\n",
      "epoch: 95/100\n",
      "loss: [1.40508167]\n",
      "------------------------------------\n",
      "epoch: 96/100\n",
      "loss: [1.38188603]\n",
      "------------------------------------\n",
      "epoch: 97/100\n",
      "loss: [1.36708768]\n",
      "------------------------------------\n",
      "epoch: 98/100\n",
      "loss: [1.38125238]\n",
      "------------------------------------\n",
      "epoch: 99/100\n",
      "loss: [1.40963609]\n",
      "------------------------------------\n",
      "epoch: 100/100\n",
      "loss: [1.42051231]\n"
     ]
    }
   ],
   "source": [
    "BP_nn.train(epoch=cyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "15ecdef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = BP_nn.get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9ce17f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(cyc), loss_train, 'g', label='Training loss')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ee313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56673ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb45647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca750f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f554f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8916cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFANN_IMP:\n",
    "    def __init__(self,X,y,lr):\n",
    "        self.inp=X\n",
    "        self.wts=[np.random.rand(self.inp.shape[1],3),\n",
    "                  np.random.rand(3,2),\n",
    "                  np.random.rand(2,1)]\n",
    "        self.lr=lr\n",
    "        self.y=y\n",
    "        self.layers=len(self.wts)-1\n",
    "        self.total_loss=list()\n",
    "        self.z=[0,0,0]\n",
    "        self.a=[0,0,0]\n",
    "        self.delta=[np.zeros((self.inp.shape[0],3)),np.zeros((self.inp.shape[0],2))]\n",
    "        self.bp_flag=1\n",
    "        \n",
    "#         self.w1=np.random.rand(self.inp.shape[1],3)\n",
    "#         self.w2=np.random.rand(3,2)\n",
    "#         self.w3=np.random.rand(2,1)\n",
    "#         self.b1=np.zeros((1,3))\n",
    "#         self.b2=np.zeros(2)\n",
    "#         self.b3=np.zeros(1)\n",
    "#         self.B3=np.random.rand(2,1)\n",
    "#         self.B2=np.random.rand(3,1)\n",
    "    \n",
    "    def FF(self):\n",
    "        xin=self.inp\n",
    "        for i in range(self.layers+1):\n",
    "            self.z[i]=np.matmul(xin,self.wts[i])\n",
    "            self.a[i]=self.f_sigmoid(self.z[i])\n",
    "            xin=self.a[i]\n",
    "        \n",
    "        # self.a[2] will be the final output *************************\n",
    "        \n",
    "#         self.z1=np.matmul(self.inp,self.w1)\n",
    "#         self.a1=self.f_sigmoid(self.z1)\n",
    "        \n",
    "#         self.z2=np.matmul(self.a1,self.w2)\n",
    "#         self.a2=self.f_sigmoid(self.z2)\n",
    "        \n",
    "#         self.z3=np.matmul(self.a2,self.w3)\n",
    "#         self.out=self.f_sigmoid(self.z3)\n",
    "    \n",
    "    def loss(self):\n",
    "        self.e=self.a[2]-self.y\n",
    "        self.total_loss.append(np.sum(self.e**2))\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return self.total_loss\n",
    "        \n",
    "    def BP(self):\n",
    "        if self.bp_flag==1:\n",
    "            B=np.random.rand(2,1)\n",
    "            self.delta[1]=self.df_sigmoid(self.z[1])*np.matmul(self.e,B.T)\n",
    "            self.bp_flag=0\n",
    "        else:\n",
    "            B=np.random.rand(3,1)\n",
    "            self.delta[0]=self.df_sigmoid(self.z[0])*np.matmul(self.e,B.T)\n",
    "            self.bp_flag=1\n",
    "            \n",
    "#         self.d2=self.df_sigmoid(self.z2)*np.matmul(self.e,self.B3.T)\n",
    "#         self.d1=self.df_sigmoid(self.z1)*np.matmul(self.e,self.B2.T)\n",
    "        \n",
    "    def update(self):\n",
    "        self.wts[2]-=self.lr*np.matmul(self.a[1].T,self.e)\n",
    "        if self.bp_flag==1:\n",
    "            self.wts[1]-=self.lr*np.matmul(self.a[0].T, self.delta[1])\n",
    "        else:\n",
    "#             print(self.inp.T.shape,self.delta[0].shape)\n",
    "            self.wts[0]-=self.lr*np.matmul(self.inp.T, self.delta[0])\n",
    "            \n",
    "#         self.b3 = self.b3-self.lr*self.e\n",
    "#         self.b2 = self.b2-self.lr*self.d2\n",
    "#         self.b1 = self.b1-self.lr*self.d1\n",
    "        \n",
    "    def f_sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def df_sigmoid(self,x):\n",
    "        return self.f_sigmoid(x)*(1-self.f_sigmoid(x))\n",
    "    \n",
    "    def train(self, itr=1000):\n",
    "        for i in range(itr):\n",
    "            self.FF()\n",
    "            self.loss()\n",
    "            self.BP()\n",
    "            self.update()\n",
    "    \n",
    "    def predict(self,x_test):\n",
    "        xin=x_test\n",
    "        for i in range(self.layers+1):\n",
    "            z_pred=np.matmul(xin,self.wts[i])\n",
    "            a_pred=self.f_sigmoid(z_pred)\n",
    "            xin=a_pred\n",
    "        \n",
    "        return xin\n",
    "        \n",
    "#         z1_pred=np.matmul(x_test,self.w1)\n",
    "#         a1_pred=self.f_sigmoid(z1_pred)\n",
    "        \n",
    "#         z2_pred=np.matmul(a1_pred,self.w2)\n",
    "#         a2_pred=self.f_sigmoid(z2_pred)\n",
    "        \n",
    "#         z3_pred=np.matmul(a2_pred,self.w3)\n",
    "#         y_pred=self.f_sigmoid(z3_pred)\n",
    "        \n",
    "#         return y_pred\n",
    "    \n",
    "    def show(self):\n",
    "        print(f'w1: {self.wts[0].shape}')\n",
    "        print(f'w2: {self.wts[1].shape}')        \n",
    "        print(f'w3: {self.wts[2].shape}')        \n",
    "#         print(f'b1: {self.b1.shape}')        \n",
    "#         print(f'b2: {self.b2.shape}')        \n",
    "#         print(f'b2: {self.b3.shape}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff74dd3",
   "metadata": {},
   "source": [
    "# DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9644caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFANN:\n",
    "    def __init__(self,X,y,lr):\n",
    "        self.inp=X\n",
    "        self.w1=np.random.rand(self.inp.shape[1],3)\n",
    "        self.w2=np.random.rand(3,2)\n",
    "        self.w3=np.random.rand(2,1)\n",
    "#         self.b1=np.zeros((1,3))\n",
    "#         self.b2=np.zeros(2)\n",
    "#         self.b3=np.zeros(1)\n",
    "        self.lr=lr\n",
    "        self.y=y\n",
    "        self.out=np.zeros(y.shape)\n",
    "        self.l=list()\n",
    "        self.B3=np.random.rand(2,1)\n",
    "        self.B2=np.random.rand(3,1)\n",
    "    \n",
    "    def FF(self):\n",
    "        self.z1=np.matmul(self.inp,self.w1)\n",
    "        self.a1=self.f_sigmoid(self.z1)\n",
    "        \n",
    "        self.z2=np.matmul(self.a1,self.w2)\n",
    "        self.a2=self.f_sigmoid(self.z2)\n",
    "        \n",
    "        self.z3=np.matmul(self.a2,self.w3)\n",
    "        self.out=self.f_sigmoid(self.z3)\n",
    "    \n",
    "    def loss(self):\n",
    "        self.e=self.out-self.y\n",
    "        self.l.append(np.sum(self.e**2))\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return self.l\n",
    "        \n",
    "    def BP(self):\n",
    "        self.d2=self.df_sigmoid(self.z2)*np.matmul(self.e,self.B3.T)\n",
    "        self.d1=self.df_sigmoid(self.z1)*np.matmul(self.e,self.B2.T)\n",
    "        \n",
    "    def update(self):\n",
    "        self.w3 = self.w3-self.lr*np.matmul(self.a2.T,self.e)\n",
    "        self.w2 = self.w2-self.lr*np.matmul(self.a1.T, self.d2)\n",
    "        self.w1 = self.w1-self.lr*np.matmul(self.inp.T, self.d1)\n",
    "#         self.b3 = self.b3-self.lr*self.e\n",
    "#         self.b2 = self.b2-self.lr*self.d2\n",
    "#         self.b1 = self.b1-self.lr*self.d1\n",
    "        \n",
    "    def f_sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def df_sigmoid(self,x):\n",
    "        return self.f_sigmoid(x)*(1-self.f_sigmoid(x))\n",
    "    \n",
    "    def train(self, itr=100):\n",
    "        for i in range(itr):\n",
    "            self.FF()\n",
    "            self.loss()\n",
    "            self.BP()\n",
    "            self.update()\n",
    "    \n",
    "    def predict(self,x_test):\n",
    "        z1_pred=np.matmul(x_test,self.w1)\n",
    "        a1_pred=self.f_sigmoid(z1_pred)\n",
    "        \n",
    "        z2_pred=np.matmul(a1_pred,self.w2)\n",
    "        a2_pred=self.f_sigmoid(z2_pred)\n",
    "        \n",
    "        z3_pred=np.matmul(a2_pred,self.w3)\n",
    "        y_pred=self.f_sigmoid(z3_pred)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def show(self):\n",
    "        print(f'w1: {self.w1.shape}')\n",
    "        print(f'w2: {self.w2.shape}')        \n",
    "        print(f'w3: {self.w3.shape}')        \n",
    "#         print(f'b1: {self.b1.shape}')        \n",
    "#         print(f'b2: {self.b2.shape}')        \n",
    "#         print(f'b2: {self.b3.shape}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5ff54",
   "metadata": {},
   "source": [
    "# DFA_newAlgo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05740319",
   "metadata": {},
   "source": [
    "# Something good probably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "141d03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFANN_X:\n",
    "    def __init__(self,X,y,lr):\n",
    "        self.inp=X\n",
    "        self.w1=np.random.rand(self.inp.shape[1],3)\n",
    "        self.w2=np.random.rand(3,2)\n",
    "        self.w3=np.random.rand(2,1)\n",
    "#         self.b1=np.zeros((1,3))\n",
    "#         self.b2=np.zeros(2)\n",
    "#         self.b3=np.zeros(1)\n",
    "        self.lr=lr\n",
    "        self.y=y\n",
    "        self.out=np.zeros(y.shape)\n",
    "        self.l=list()\n",
    "        self.B3=np.random.rand(2,1)\n",
    "        self.B2=np.random.rand(3,1)\n",
    "        self.ac=[0,0]\n",
    "    \n",
    "    def FF(self):\n",
    "        self.z1=np.matmul(self.inp,self.w1)\n",
    "        self.a1=self.f_sigmoid(self.z1)\n",
    "        \n",
    "        self.z2=np.matmul(self.a1,self.w2)\n",
    "        self.a2=self.f_sigmoid(self.z2)\n",
    "        \n",
    "        self.z3=np.matmul(self.a2,self.w3)\n",
    "        self.out=self.f_sigmoid(self.z3)\n",
    "    \n",
    "    def loss(self):\n",
    "        self.e=self.out-self.y\n",
    "        self.l.append(np.sum(self.e**2))\n",
    "        \n",
    "    def get_loss(self):\n",
    "        return self.l\n",
    "        \n",
    "    def BP(self):\n",
    "        self.w3 = self.w3-self.lr*np.matmul(self.a2.T,self.e)\n",
    "        if np.sum(self.z2)>np.sum(self.z1):\n",
    "            self.d2=self.df_sigmoid(self.z2)*np.matmul(self.e,self.B3.T)\n",
    "            self.w2-=self.lr*np.matmul(self.a1.T, self.d2)\n",
    "            self.ac[1]+=1\n",
    "        else:\n",
    "            self.d1=self.df_sigmoid(self.z1)*np.matmul(self.e,self.B2.T)\n",
    "            self.w1-=self.lr*np.matmul(self.inp.T, self.d1)\n",
    "            self.ac[0]+=1\n",
    "    \n",
    "    def count_act(self):\n",
    "        return self.ac\n",
    "        \n",
    "#     def update(self):\n",
    "#         self.w3 = self.w3-self.lr*np.matmul(self.a2.T,self.e)\n",
    "#         self.w2 = self.w2-self.lr*np.matmul(self.a1.T, self.d2)\n",
    "#         self.w1 = self.w1-self.lr*np.matmul(self.inp.T, self.d1)\n",
    "#         self.b3 = self.b3-self.lr*self.e\n",
    "#         self.b2 = self.b2-self.lr*self.d2\n",
    "#         self.b1 = self.b1-self.lr*self.d1\n",
    "        \n",
    "    def f_sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def df_sigmoid(self,x):\n",
    "        return self.f_sigmoid(x)*(1-self.f_sigmoid(x))\n",
    "    \n",
    "    def train(self, itr=100):\n",
    "        for i in range(itr):\n",
    "            self.FF()\n",
    "            self.loss()\n",
    "            self.BP()\n",
    "#             self.update()\n",
    "    \n",
    "    def predict(self,x_test):\n",
    "        z1_pred=np.matmul(x_test,self.w1)\n",
    "        a1_pred=self.f_sigmoid(z1_pred)\n",
    "        \n",
    "        z2_pred=np.matmul(a1_pred,self.w2)\n",
    "        a2_pred=self.f_sigmoid(z2_pred)\n",
    "        \n",
    "        z3_pred=np.matmul(a2_pred,self.w3)\n",
    "        y_pred=self.f_sigmoid(z3_pred)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def show(self):\n",
    "        print(f'w1: {self.w1.shape}')\n",
    "        print(f'w2: {self.w2.shape}')        \n",
    "        print(f'w3: {self.w3.shape}')        \n",
    "#         print(f'b1: {self.b1.shape}')        \n",
    "#         print(f'b2: {self.b2.shape}')        \n",
    "#         print(f'b2: {self.b3.shape}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b56daaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'x1':[0, 0, 1, 0],\n",
    "#         'x2':[0, 1, 1, 1],\n",
    "#         'x3':[1, 0, 1, 1],\n",
    "#         'y':[1, 1, 1, 0]}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# X_train = df.iloc[:,:3].to_numpy()\n",
    "# y_train = df.iloc[:,3:4].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "20011a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=pd.read_csv('Data/train.csv')\n",
    "# ind=np.random.randint(0,371,100)\n",
    "# train_data=train_data.iloc[ind,:]\n",
    "# train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3cf67ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=train_data[['bone_length','rotting_flesh','hair_length','has_soul']].to_numpy()\n",
    "# y=train_data['type'].to_numpy()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=25, test_size=0.25)\n",
    "# y_train=y_train.reshape(-1,1)\n",
    "# y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10258a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogation\n",
    "cyc=10\n",
    "BP_nn=BPNN(X_train,y_train,lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2d0c1ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'itr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-da1f3dd31255>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBP_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcyc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'itr'"
     ]
    }
   ],
   "source": [
    "BP_nn.train(itr=cyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80134fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = BP_nn.get_loss()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(cyc), loss_train, 'g', label='Training loss')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1af3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('E:/Reseach/NewAlgo/Data/MNIST/train/train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad4ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8732b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cf76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFA\n",
    "cyc=10000\n",
    "DFA_nn=DFANN(X_train,y_train,lr=0.01)\n",
    "DFA_nn.train(itr=cyc)\n",
    "loss_train = DFA_nn.get_loss()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(cyc), loss_train, 'g', label='Training loss')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DFA_newAlgo\n",
    "cyc=30000\n",
    "DFA_nn_imp=DFANN_IMP(X_train,y_train,lr=0.05)\n",
    "DFA_nn_imp.train(itr=cyc)\n",
    "loss_train = DFA_nn_imp.get_loss()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(cyc), loss_train, 'g', label='Training loss')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e99269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGP\n",
    "cyc=10000\n",
    "DFA_nn_X=DFANN_X(X_train,y_train,lr=0.1)\n",
    "DFA_nn_X.train(itr=cyc)\n",
    "loss_train = DFA_nn_X.get_loss()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(cyc), loss_train, 'g', label='Training loss')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFA_nn_X.count_act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e477e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGP\n",
    "cyc=10000\n",
    "DFA_nn_X=DFANN_X(X_train,y_train,lr=0.1)\n",
    "DFA_nn_X.train(itr=cyc)\n",
    "loss_train = DFA_nn_X.get_loss()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(cyc), loss_train, 'g', label='Training loss')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFA_nn_X.count_act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogation\n",
    "cyc=10000\n",
    "BP_nn=BPNN(X_train,y_train,lr=0.005)\n",
    "BP_nn.train(itr=cyc)\n",
    "loss_train = BP_nn.get_loss()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(cyc), loss_train, 'g', label='Training loss')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7c2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
